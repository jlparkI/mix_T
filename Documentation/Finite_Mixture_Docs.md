FiniteStudentMixture
===================

 - [Parameters](#Parameters)
 - [Methods](#Methods)
 - [Example](#Example)

```python
import studenttmixture
from studenttmixture.finite_mixture import FiniteStudentMixture

FiniteStudentMixture(n_components=2, tol=1e-3,
reg_covar=1e-6, max_iter=500, n_init=1, df=4, fixed_df=True,
random_state=123, verbose=False)
```

### Parameters

  * *n_components*<br>The number of components in the mixture.
  * *tol*<br>The threshold for change in complete data log likelihood to achieve convergence.
  * *reg_covar*<br> A small floating point value > 0 added to the diagonal of the scale matrix
 for each component to ensure it is positive definite.
  * *max_iter*<br>The maximum number of iterations.
  * *n_init*<br>The EM algorithm converges on a local maximum, so re-running the algorithm several
times with different starting points may improve the chances of obtaining an optimal outcome. n_init
determines the number of re-initializations to run.
  * *df*<br>The starting value for degrees of freedom for each component. If fixed_df is True,
all components use a fixed degrees of freedom with the specified value; if it is False,
this starting value is optimized during fitting.
  * *fixed_df*<br>Whether to optimize degrees of freedom or use a fixed user-specified value. 
Optimizing df may provide a better fit and may be informative but also tends to result in slower
convergence.
  * *random_state*<br>The seed for the random number generator.
  * *verbose*<br>Whether to print the lower bound and change in lower bound during fitting.



### Methods

*Note that for all methods, the input X is a numpy array where each row is a datapoint and each column is
a feature.*

| Method     | Description |
| ---------- | ----------- |
| aic(X)     | j           |
    aic(X)
Returns the Akaike information criterion for the input X. A lower value is better. Varying the number of
components and choosing the number that yields the smallest AIC is one approach to choosing an appropriate number
of clusters.

    bic(X)
Returns the Bayes information criterion for the input X. A lower value is better. Varying the number of
components and choosing the number that yields the smallest BIC is one approach to choosing an appropriate number
of clusters. Note that BIC penalizes the number of parameters much more strongly than AIC and is therefore more
conservative.

    fit(X)
Fit the model to the input data X. Yields a value error with appropriate warnings if the fit does not converge.

    fit_predict(X)
Returns a numpy array of predictions for the input data (after first fitting the model). Each element i is
an integer in [0,n_components - 1]. These predictions are "hard" assignments obtained by assigning each
datapoint to the cluster for which it has the largest posterior probability (see *predict_proba* below).

    predict(X)
Returns a numpy array of predictions for the input data. Can only be called for a model that has already been
fitted. Each element i is an integer in [0,n_components - 1]. These predictions are "hard" assignments obtained
by assigning each datapoint to the cluster for which it has the largest posterior probability (see 
*predict_proba* below).

    predict_proba(X)
Returns a numpy array of dimensions (NxK) for N input datapoints and K components, where each element ij 
is the posterior probability that sample i belongs to cluster j. Assigning each datapoint to the most probable
cluster yields the "hard" cluster assignments generated by the *predict* and *fit_predict* functions.

    score(X)
Returns the average (over datapoints) log likelihood of the input data. When n_init is > 1, during fitting,
the algorithm chooses to keep the model with the largest score.

    score_samples(X)
Returns the weighted log likelihood for each datapoint.

    get_cluster_centers()
Returns a numpy array of dimensions (KxD), where K is the number of components and D is the dimensionality of
the data used to fit the model, containing the location of each cluster. Note that the "location" for a t-
distribution is analogous to the mean of a Gaussian distribution, i.e it denotes the center of
the distribution, but should not be called the mean because it is not the mean of the data.

    get_cluster_scales()
Returns a numpy array of dimensions (DxDxK), where D is the dimensionality of the data used to fit the model
and K is the number of components. Each (DxD) matrix is a scale matrix for one of the components. Note that the
"scale" matrix for a t-distribution is analogous to the covariance matrix of a Gaussian distribution but should
not be called the "covariance" because it is not the covariance of the data.

    get_weights()
Returns an array of shape K, where K is the number of components. Each element of this array is the mixture
weight for that component.

    get_df()
Returns an array of shape K, where K is the number of components. Each element of this array is the degrees
of freedom for that component. If you specify *fixed_df=True* when creating the FiniteStudentMixture object,
degrees of freedom will not be optimized -- the value you supplied for *df* (default 4) will be used, 
so all clusters will have the same df in that case.


### Example

```python
#This is a fairly easy clustering problem with well-separated clusters
#(in other words, unlike any real-world dataset!) However, a Gaussian
#mixture performs much worse than a studenttmixture just because
#of a handful of scattered outliers, demonstrating the advantages
#of clustering using a mixture of t-distributions.

import studenttmixture
from studenttmixture.finite_mixture import FiniteStudentMixture
import matplotlib.pyplot as plt, numpy as np

#Note that scikit-learn is not a dependency and is not
#required for studenttmixture; we're only using it here to get
#some convenient toy datasets to demonstrate use of this package, and
#to compare the results of a Gaussian mixture model with a 
#studenttmixture.
import sklearn
from sklearn.datasets import make_blobs

#We generate a dataset with 3 blobs and some randomly scattered 
#outlier points.
x_clusters, y, centers = make_blobs(n_samples=100, 
                           return_centers=True, random_state=128)
np.random.seed(123)
x_noise = np.random.uniform(low=3*np.min(x_clusters), 
                            high=3*np.max(x_clusters),
                           size=(10,2))
x = np.vstack([x_clusters, x_noise])
plt.scatter(x[:,0], x[:,1], s=10)
plt.title("Unclustered data")
plt.show()
```
![raw_data](https://github.com/jlparkI/mix_T/blob/main/Documentation/Unclustered_data.png)

```python
#We choose the number of clusters using information criteria. Notice that when doing many different fits
#as in this case with different numbers of clusters (some of which are highly non-optimal) 
#it is often better to use fixed_df with a low value. 
#We can optimize df later once we have selected the number of clusters.
aics, bics = [], []
for ncomp in range(2,10):
    mix_model = FiniteStudentMixture(n_components=ncomp, fixed_df=True, df=1.0)
    mix_model.fit(x)
    aics.append(mix_model.aic(x))
    bics.append(mix_model.bic(x))

plt.scatter(np.arange(2, 10), aics, color="red", label="AIC")
plt.scatter(np.arange(2,10), bics, color="blue", label="BIC")
plt.title("Information criteria vs number of clusters for toy dataset")
plt.xlabel("Num clusters")
#Consistent with expectations (in this case), we see an "elbow" for AIC and a clear minimum for BIC at
#3 clusters. The absolute value of these information criteria is not important, the location of the "elbow"
#or minimum is. Note that BIC discriminates much more heavily against models with more parameters.
```
![bic_aic](https://github.com/jlparkI/mix_T/blob/main/Documentation/AIC_BIC.png)

```python
#We fit both scikitlearn's Gaussian mixture and the FiniteStudentMixture using 3 clusters and 
#compare the results.
from sklearn.mixture import GaussianMixture as GMM
gm = GMM(n_components=3, n_init=5, random_state=11)
stm = FiniteStudentMixture(n_components=3, n_init=5, fixed_df=False, df=1, random_state=11)
gm.fit(x)
stm.fit(x)
plt.scatter(x[:,0], x[:,1], s=10, label="raw data")
stm_clusters = stm.get_cluster_centers()
plt.scatter(gm.means_[:,0], gm.means_[:,1], color="red", marker="*", 
                label="Gaussian mixture", alpha=0.5, s=100)
plt.scatter(stm_clusters[:,0], stm_clusters[:,1], color="black", marker="X",
               label="Student's t-mixture", s=100)
    
plt.legend()

#Notice that the Gaussian mixture model behaves unpredictably and erratically in the presence
#of outliers, even on a very simple toy dataset. A Student's t-mixture, by contrast, performs
#just fine. Obviously, real-world datasets pose additional challenges not explored here, and
#in many cases other clustering techniques may be more suitable, but the point is that
#a Student's t-mixture is often a more appropriate technique than a Gaussian mixture.
#In the best case scenario, it will beat the Gaussian mixture by a substantial margin;
#worst case, it may give you similar results.
```
![comparison](https://github.com/jlparkI/mix_T/blob/main/Documentation/STM_vs_GMM.png)

