EMStudentMixture
===================

 - [Parameters](#Parameters)
 - [Attributes](#Attributes)
 - [Methods](#Methods)
 - [Example](#Example)

```python
import studenttmixture
from studenttmixture import EMStudentMixture

EMStudentMixture(n_components=2, tol=1e-3,
reg_covar=1e-6, max_iter=1000, n_init=1, df=4, fixed_df=True,
random_state=123, verbose=False, init_type="k++")
```

### Parameters

  * *n_components*<br>The number of components in the mixture.
  * *tol*<br>The threshold for change in complete data log likelihood to achieve convergence.
  * *reg_covar*<br> A small floating point value > 0 added to the diagonal of the scale matrix
 for each component to ensure it is positive definite.
  * *max_iter*<br>The maximum number of iterations.
  * *n_init*<br>The EM algorithm converges on a local maximum, so re-running the algorithm several
times with different starting points may improve the chances of obtaining an optimal outcome. n_init
determines the number of re-initializations to run.
  * *df*<br>The starting value for degrees of freedom for each component. If fixed_df is True,
all components use a fixed degrees of freedom with the specified value; if it is False,
this starting value is optimized during fitting.
  * *fixed_df*<br>Whether to optimize degrees of freedom or use a fixed user-specified value. 
Optimizing df may provide a better fit and may be informative but also tends to result in slower
convergence.
  * *random_state*<br>The seed for the random number generator.
  * *verbose*<br>Whether to print the lower bound and change in lower bound during fitting.
  * *init_type*<br>Must be one of "k++" or "kmeans". If "k++", the component locations are initialized
the KMeans++ procedure described by Arthur and Vassilvitskii (2007), which generally gives reasonably good
starting locations. If "kmeans", these starting locations are further refined using kmeans clustering. This is
slower because now the data is clustered twice -- first using kmeans to get starting points, then again using
EM -- and often does not give a huge advantage over k++, but in some situations it may be more effective.


### Attributes

| Attribute     | Description |
| ---------- | ----------- |
| location     | A K x D array for K components, D dimensions where each row k is the location (analogous to mean of a Gaussian) for component k. |
| scale | A D x D x K array for K components, D dimensions where each D x D array is the scale (analogous to covariance matrix for a Gaussian) matrix for component k. |
| degrees_of_freedom | A size K array for K components where each element is the degrees of freedom for component k. If you specify fixed_df = True, the starting value is used and is not optimized. fixed_df = False, by contrast, will ensure that degrees_of_freedom is optimized. |
| mix_weights | A size K array for K components where each element k is the weight of each mixture component (the probability that a random draw is from component k). |


### Methods

*Note that for all methods, the input X is a numpy array where each row is a datapoint and each column is
a feature.*

| Method     | Description |
| ---------- | ----------- |
| aic(X)     | Returns the Akaike information criterion for the input X. A lower value is better. Varying the number of components and choosing the number that yields the smallest AIC is one approach to choosing the number of clusters. |
| bic(X)     | Returns the Bayes information criterion for the input X. A lower value is better. Varying the number of components and choosing the number that yields the smallest BIC is one approach to choosing an appropriate number of clusters. Note that BIC penalizes the number of parameters much more strongly than AIC and is therefore more conservative. |
| fit(X)     | Fit the model to the input data X. |
| fit_predict(X) | Returns a numpy array of predictions for the input data (after first fitting the model). Each prediction is an integer in [0,n_components-1]. These predictions are "hard" assignments obtained by assigning each datapoint to the cluster for which it has the largest posterior probability (see *predict_proba* below). |
| predict(X) | Returns a numpy array of predictions for the input data. Can only be called for a model that has already been fitted. Each element i is an integer in [0,n_components-1] and is generated by assigning each datapoint to the cluster for which it has the largest posterior probability. |
| predict_proba(X) | Returns a numpy array of dimensions (NxK) for N input datapoints and K components, where each element ij is the posterior probability that sample i belongs to cluster j. Assigning each datapoint to the cluster for which it has the highest posterior probability yields the "hard" assignments generated by *predict* and *fit_predict*. |
| score(X)   | Returns the average (over datapoints) log likelihood of the input data. When n_init is > 1, during fitting, the algorithm chooses to keep the set of parameters that yield the largest score. |
| score_samples(X) | Returns the weighted log likelihood for each datapoint. |
| sample(num_samples = 1, random_seed=123) | Generates samples from a fitted EMStudentMixture, using the random number generator seed you provide. |


### Example

```python
#This is a fairly easy clustering problem with well-separated clusters
#(in other words, unlike any real-world dataset!) However, a Gaussian
#mixture performs much worse than a studenttmixture just because
#of a handful of scattered outliers, demonstrating the advantages
#of clustering using a mixture of t-distributions.

import studenttmixture
from studenttmixture import EMStudentMixture
import matplotlib.pyplot as plt, numpy as np

#Note that scikit-learn is not a dependency and is not
#required for studenttmixture; we're only using it here to get
#some convenient toy datasets to demonstrate use of this package, and
#to compare the results of a Gaussian mixture model with a 
#studenttmixture.
import sklearn
from sklearn.datasets import make_blobs

#We generate a dataset with 3 blobs and some randomly scattered 
#outlier points.
x_clusters, y, centers = make_blobs(n_samples=100, 
                           return_centers=True, random_state=128)
np.random.seed(123)
x_noise = np.random.uniform(low=3*np.min(x_clusters), 
                            high=3*np.max(x_clusters),
                           size=(10,2))
x = np.vstack([x_clusters, x_noise])
plt.scatter(x[:,0], x[:,1], s=10)
plt.title("Unclustered data")
plt.show()
```
![raw_data](https://github.com/jlparkI/mix_T/blob/main/Documentation/Unclustered_data.png)

```python
#We choose the number of clusters using information criteria. Notice that when doing many different fits
#as in this case with different numbers of clusters (some of which are highly non-optimal) 
#it is often better to use fixed_df with a low value. 
#We can optimize df later once we have selected the number of clusters.
aics, bics = [], []
for ncomp in range(2,10):
    mix_model = FiniteStudentMixture(n_components=ncomp, fixed_df=True, df=1.0)
    mix_model.fit(x)
    aics.append(mix_model.aic(x))
    bics.append(mix_model.bic(x))

plt.scatter(np.arange(2, 10), aics, color="red", label="AIC")
plt.scatter(np.arange(2,10), bics, color="blue", label="BIC")
plt.title("Information criteria vs number of clusters for toy dataset")
plt.xlabel("Num clusters")
#Consistent with expectations (in this case), we see an "elbow" for AIC and a clear minimum for BIC at
#3 clusters. The absolute value of these information criteria is not important, the location of the "elbow"
#or minimum is. Note that BIC discriminates much more heavily against models with more parameters.
```
![bic_aic](https://github.com/jlparkI/mix_T/blob/main/Documentation/AIC_BIC.png)

```python
#We fit both scikitlearn's Gaussian mixture and the FiniteStudentMixture using 3 clusters and 
#compare the results.
from sklearn.mixture import GaussianMixture as GMM
gm = GMM(n_components=3, n_init=5, random_state=11)
stm = FiniteStudentMixture(n_components=3, n_init=5, fixed_df=False, df=1, random_state=11)
gm.fit(x)
stm.fit(x)
plt.scatter(x[:,0], x[:,1], s=10, label="raw data")
stm_clusters = stm.get_cluster_centers()
plt.scatter(gm.means_[:,0], gm.means_[:,1], color="red", marker="*", 
                label="Gaussian mixture", alpha=0.5, s=100)
plt.scatter(stm_clusters[:,0], stm_clusters[:,1], color="black", marker="X",
               label="Student's t-mixture", s=100)
    
plt.legend()

#Notice that the Gaussian mixture model behaves unpredictably and erratically in the presence
#of outliers, even on a very simple toy dataset. A Student's t-mixture, by contrast, performs
#just fine. Obviously, real-world datasets pose additional challenges not explored here, and
#in many cases other clustering techniques may be more suitable, but the point is that
#a Student's t-mixture is often a more appropriate technique than a Gaussian mixture.
#In the best case scenario, it will beat the Gaussian mixture by a substantial margin;
#worst case, it may give you similar results.
```
![comparison](https://github.com/jlparkI/mix_T/blob/main/Documentation/STM_vs_GMM.png)

