FiniteStudentMixture
===================

 - [Parameters](#Parameters)
 - [Methods](#Methods)
 - [Example](#Example)

```python
import studenttmixture
from studenttmixture.finite_mixture import FiniteStudentMixture

FiniteStudentMixture(n_components=2, tol=1e-3,
reg_covar=1e-6, max_iter=500, n_init=1, df=4, fixed_df=True,
random_state=123, verbose=False)
```

#### Parameters

  * *n_components*<br>The number of components in the mixture.
  * *tol*<br>The threshold for change in complete data log likelihood to achieve convergence.
  * *reg_covar*<br> A small floating point value > 0 added to the diagonal of the scale matrix
 for each component to ensure it is positive definite.
  * *max_iter*<br>The maximum number of iterations.
  * *n_init*<br>The EM algorithm converges on a local maximum, so re-running the algorithm several
times with different starting points may improve the chances of obtaining an optimal outcome. n_init
determines the number of re-initializations to run.
  * *df*<br>The starting value for degrees of freedom for each component. If fixed_df is True,
all components use a fixed degrees of freedom with the specified value; if it is False,
this starting value is optimized during fitting.
  * *fixed_df*<br>Whether to optimize degrees of freedom or use a fixed user-specified value. 
Optimizing df may provide a better fit and may be informative but also tends to result in slower
convergence.
  * *random_state*<br>The seed for the random number generator.
  * *verbose*<br>Whether to print the lower bound and change in lower bound during fitting.

#### Methods

*Note that for all methods, the input X is a numpy array where each row is a datapoint and each column is
a feature.*

    aic(X)<br>
Returns the Akaike information criterion for the input X. A lower value is better. Varying the number of
components and choosing the number that yields the smallest AIC is one approach to choosing an appropriate number
of clusters.

    bic(X)<br>
Returns the Bayes information criterion for the input X. A lower value is better. Varying the number of
components and choosing the number that yields the smallest BIC is one approach to choosing an appropriate number
of clusters. Note that BIC penalizes the number of parameters much more strongly than AIC and is therefore more
conservative.

    fit(X)<br>
Fit the model to the input data X. Yields a value error with appropriate warnings if the fit does not converge.

    fit_predict(X)<br>
Returns a numpy array of predictions for the input data (after first fitting the model). Each element i is
an integer in [0,n_components - 1]. These predictions are "hard" assignments obtained by assigning each
datapoint to the cluster for which it has the largest posterior probability (see *predict_proba* below).

    predict(X)<br>
Returns a numpy array of predictions for the input data. Can only be called for a model that has already been
fitted. Each element i is an integer in [0,n_components - 1]. These predictions are "hard" assignments obtained
by assigning each datapoint to the cluster for which it has the largest posterior probability (see 
*predict_proba* below).

    predict_proba(X)<br>
Returns a numpy array of dimensions (NxK) for N input datapoints and K components, where each element ij 
is the posterior probability that sample i belongs to cluster j. Assigning each datapoint to the most probable
cluster yields the "hard" cluster assignments generated by the *predict* and *fit_predict* functions.

    score(X)<br>
Returns the average (over datapoints) log likelihood of the input data. When n_init is > 1, during fitting,
the algorithm chooses to keep the model with the largest score.

    score_samples(X)<br>
Returns the weighted log likelihood for each datapoint.

    get_cluster_centers()<br>
Returns a numpy array of dimensions (KxD), where K is the number of components and D is the dimensionality of
the data used to fit the model, containing the location of each cluster. Note that the "location" for a t-
distribution is analogous to the mean of a Gaussian distribution, i.e it denotes the center of
the distribution, but should not be called the mean because it is not the mean of the data.

    get_cluster_scales()<br>
Returns a numpy array of dimensions (DxDxK), where D is the dimensionality of the data used to fit the model
and K is the number of components. Each (DxD) matrix is a scale matrix for one of the components. Note that the
"scale" matrix for a t-distribution is analogous to the covariance matrix of a Gaussian distribution but should
not be called the "covariance" because it is not the covariance of the data.

    get_weights()<br>
Returns an array of shape K, where K is the number of components. Each element of this array is the mixture
weight for that component.

    get_df()<br>
Returns an array of shape K, where K is the number of components. Each element of this array is the degrees
of freedom for that component. If you specify *fixed_df=True* when creating the FiniteStudentMixture object,
degrees of freedom will not be optimized -- the value you supplied for *df* (default 4) will be used, 
so all clusters will have the same df in that case.

#### Example


